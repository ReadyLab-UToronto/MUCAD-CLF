# The Multi-User Computer-Aided Design Collaborative Learning Framework (MUCAD-CLF) 
Code available in this repository aims to provide an efficient tool to visualize and compare users' audit trail data from [Onshape Enterprise Analytics](https://www.onshape.com/en/features/analytics). 

## Table of contents
* [General Info](#general-info)
* [Technologies](#technologies)
* [Setup](#setup)
* [Maintainers](#maintainers)
* [Contributing](#contributing) 

## General Info 
As Onshape Analytics provides a rich but enormous dataset, logging all user actions in an Onshape document, visualizing the data and potentially compare the data between different users efficiently are often challenging. We first developed two action classification methods in this framework to organize and count the recorded action data in several meaningful categories. Then, efforts were spent on creatively plotting the counts for visaulization and comparison. 

Structure of the repository: 
* `action_classification` contains the two action classification methods that each organize actions in six different categories 
* `action_counting` takes in an audit trail file in CSV format and count the actions using the two action classification methods 
* `action_count_plotting` provides a few plotting functions that may help the visualization and comparison of the data 
* `sample_audit_trails` contains several sample audit trails generated by us for demonstration 
* `tests` provides a demonstrating script on the usage of this project along with the sample outputs 

## Technologies
Project is created with:
* Python 3.6
	
## Setup
To use this project, please read the following steps: 
1. Download this repository as you would for any other GitHub project 
2. Download all relevant audit trails for analysis from your Onshape Enterprise account in CSV format 
3. Rename the CSV files if necessary (file names will be used to format output) 
4. Place all unzipped audit trial files into the `sample_audit_trails` folder 
5. Read, modify, and run `test_aggregate_count.py` in the `tests` folder for a quick demonstration in any Python IDE of your choice 
6. Aggregate actions count will be recorded in `Counts.csv` in the `tests` folder, and plots will be shown as specified in `test_aggregate_count.py` 

## Maintainers
* Yuanzhe (Felix) Deng - yuanzhe.deng@mail.utoronto.ca 
* Alison Olechowski - olechowski@mie.utoronto.ca 

Supporting organization: 
* Ready Lab, University of Toronto - https://readylab.mie.utoronto.ca 

## Contributing 
We welcome and appreciate future contributions to the framework from the Onshape users, educators, and researchers community, especially in the following categories: 
* New action classification methods 
* New plots/methods to visualize the data and compare data between users 
* Other useful data to be collected from the users, uncaptured by Onshape Analytics  

To contribute, please provide the code for the new method/idea along with a testing script. Then, contact the maintainers and submit a pull request.  
